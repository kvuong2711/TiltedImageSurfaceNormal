---
layout: default
title: Surface Normal Estimation of Tilted Images via Spatial Rectifier
---
<div class="title">
  <h1 class="title-name">
    <strong>Surface Normal Estimation of Tilted Images via Spatial Rectifier</strong>
  </h1>
</div>

<div class="eccv-conf">
  <h4>ECCV 2020 Spotlight</h4>
</div>

<div class="authors">
  <div>Tien Do</div>
  <div>Khiem Vuong</div>
  <div>Stergios Roumeliotis</div>
  <div>Hyun Soo Park</div>
</div>

<p style="text-align: center; font-size: 0.9em">University of Minnesota</p>

<div class="abstract">
  <h2>Abstract</h2>
  <p style="text-align: justify;">
    We present a spatial rectifier to estimate surface normal of tilted images.
    Tilted images are of particular interest as more visual data are captured by
    embodied sensors such as body-/robot-mounted cameras. Existing approaches
    exhibit bounded performance on predicting surface normal because they were
    trained by gravity-aligned images. Our two main hypotheses are: (1) visual
    scene layout is indicative of the gravity direction; and (2) not all surfaces
    are equally represented, i.e., there exists a transformation of the tilted
    image to produce better surface normal prediction. We design the spatial
    rectifier that is learned to transform the surface normal distribution of
    the tilted image such that it matches to that of gravity aligned images.
    The spatial rectifier is parametrized by a principle direction of the
    rectified image that maximizes the distribution match. When training,
    we jointly learn the gravity and principle direction that can synthesize
    images and surface normal to supervise the surface normal estimator.
    Inspired by the panoptic feature pyramid network, a new network that can
    access to both global and local visual features is proposed by leveraging
    dilated convolution in the decoder. The resulting estimator produces accurate
    surface normal estimation, outperforming state-of-the-art methods and data
    augmentation baseline. We evaluate our method not only on ScanNet and NYUv2
    but also on a new dataset called Tilt-RGBD that includes substantial roll and
    pitch camera motion captured by body-mounted cameras.
  </p>
</div>

<div class="video">
  <h2>Supplementary Video</h2>
  <div class="video-container">
    <iframe width="560" height="315" src="https://www.youtube.com/embed/RS0K-SDqTYY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </div>
</div>

<div class="image">
  <h2>Tilt-RGBD Dataset</h2>
    <img src="images/tilt-inverted.jpeg" class="single-image">
</div>

<!--<div class="image">-->
<!--  <h2>Images Testing (multiple images)</h2>-->
<!--  <div class="grid-container">-->
<!--    <div><img src="images/tilt-inverted.jpeg" class="small-image"></div>-->
<!--    <div><img src="images/dummy.jpg" class="small-image"></div>-->
<!--  &lt;!&ndash;  <div class="item3"><a href="https://arxiv.org/abs/2005.08877"><img src="images/tilt-inverted.jpeg" class="paper-thumbnail"></a></div>&ndash;&gt;-->
<!--  &lt;!&ndash;  <div class="item4"><a href="https://arxiv.org/abs/2005.08877"><img src="images/tilt-inverted.jpeg" class="paper-thumbnail"></a></div>&ndash;&gt;-->
<!--  </div>-->
<!--</div>-->
